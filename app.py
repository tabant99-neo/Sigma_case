import streamlit as st
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import tempfile
import os
import re
import numpy as np
from typing import List
import time
import gc

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞ –ø–æ —Ä—É—Å—Å–∫–æ–º—É —è–∑—ã–∫—É",
    page_icon="üá∑üá∫",
    layout="wide"
)

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ PyTorch –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
torch.set_num_threads(4)
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏
if 'processing_state' not in st.session_state:
    st.session_state.processing_state = {
        'is_processing': False,
        'current_index': 0,
        'total_rows': 0,
        'start_time': 0,
        'results': None,
        'original_df': None,
        'selected_column': None
    }

if 'graded_results' not in st.session_state:
    st.session_state.graded_results = None

if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False

if 'grader_instance' not in st.session_state:
    st.session_state.grader_instance = None

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.title("üá∑üá∫ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞ –ø–æ —Ä—É—Å—Å–∫–æ–º—É —è–∑—ã–∫—É")
st.markdown("""
–≠—Ç–æ –¥–µ–º–æ-–≤–µ—Ä—Å–∏—è —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏ –ø–∏—Å—å–º–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV-—Ñ–∞–π–ª —Å –æ—Ç–≤–µ—Ç–∞–º–∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –∏–ª–∏ –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –≤—Ä—É—á–Ω—É—é.
""")

# –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
class RussianExamGrader:
    def __init__(self, model_path):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        st.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑: {model_path}")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø—É—Ç–∏ –∫ –º–æ–¥–µ–ª–∏
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"Model path not found: {model_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
            
            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU/CPU)
            self.model.to(self.device)
            self.model.eval()
            
            st.success("‚úÖ –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            st.session_state.model_loaded = True
            
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            st.info("""
            **–†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã:**
            1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—å—é —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤: pytorch_model.bin, config.json
            3. –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –ø–∞–ø–∫–µ 'my_trained_model_2'
            """)
            raise e

    def preprocess_text(self, text):
        """–ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞."""
        text = str(text).strip()
        if not text:
            return ""
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ - –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö
        return text

    def predict(self, text):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        try:
            if not text or len(str(text).strip()) == 0:
                return 0.0
                
            processed_text = self.preprocess_text(text)
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(
                processed_text,
                max_length=512,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            ).to(self.device)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                outputs = self.model(**inputs)
                prediction = outputs.logits.cpu().numpy()

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –æ—Ü–µ–Ω–∫—É 0-5
            grade = float(prediction[0][0])
            grade = max(0, min(5, grade))
            return round(grade, 2)
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
            return 0.0

    def predict_batch(self, texts: List[str]) -> List[float]:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        try:
            processed_texts = [self.preprocess_text(text) for text in texts]
            
            # –ü–∞–∫–µ—Ç–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(
                processed_texts,
                max_length=512,
                padding=True,
                truncation=True,
                return_tensors='pt'
            ).to(self.device)

            # –ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = outputs.logits.cpu().numpy()

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Å–µ –æ—Ü–µ–Ω–∫–∏
            grades = predictions[:, 0].tolist()
            grades = [max(0, min(5, float(grade))) for grade in grades]
            return [round(grade, 2) for grade in grades]
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
            # –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –æ–¥–Ω–æ–º—É
            return [self.predict(text) for text in texts]

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è CSV —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –¥–ª—è UTF-8 –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è ';'
def safe_read_csv(uploaded_file):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ CSV —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –¥–ª—è UTF-8 –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è ';'"""
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º UTF-8 —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º ';' (–æ—Å–Ω–æ–≤–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç)
    try:
        uploaded_file.seek(0)
        df = pd.read_csv(uploaded_file, encoding='utf-8', sep=';', on_bad_lines='skip')
        if len(df.columns) > 0 and len(df) > 0:
            st.success(f"‚úÖ –§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π UTF-8 –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º ';'")
            st.info(f"üìä –î–∞–Ω–Ω—ã–µ: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")
            return df
    except Exception as e:
        st.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å UTF-8 –∏ ';': {e}")
    
    # –ó–∞—Ç–µ–º –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    encodings = ['utf-8', 'cp1251', 'windows-1251', 'iso-8859-1', 'latin1']
    separators = [',', '\t']
    
    for encoding in encodings:
        try:
            uploaded_file.seek(0)
            for sep in separators:
                try:
                    uploaded_file.seek(0)
                    df = pd.read_csv(uploaded_file, encoding=encoding, sep=sep, on_bad_lines='skip')
                    if len(df.columns) > 0 and len(df) > 0:
                        st.success(f"‚úÖ –§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")
                        st.info(f"–ö–æ–¥–∏—Ä–æ–≤–∫–∞: {encoding}, —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: '{sep}'")
                        return df
                except Exception as e:
                    continue
        except UnicodeDecodeError:
            continue
        except Exception as e:
            continue
    
    # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ —Å engine='python'
    try:
        uploaded_file.seek(0)
        df = pd.read_csv(uploaded_file, encoding='utf-8', sep=None, engine='python', on_bad_lines='skip')
        if len(df) > 0:
            st.info("–§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è")
            return df
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
    
    raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π UTF-8 –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º ';'")

# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö CSV —Ñ–∞–π–ª–æ–≤
def process_large_dataset(df, grader, selected_column, chunk_size=500):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ —á–∞—Å—Ç—è–º —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
    try:
        if selected_column not in df.columns:
            st.error(f"–°—Ç–æ–ª–±–µ—Ü '{selected_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None
        
        total_rows = len(df)
        st.info(f"üìä –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_rows} —Å—Ç—Ä–æ–∫")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –¥–ª—è UI
        progress_container = st.container()
        status_container = st.container()
        stats_container = st.container()
        
        all_grades = []
        start_time = time.time()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª –ø–æ —á–∞—Å—Ç—è–º
        for start_idx in range(0, total_rows, chunk_size):
            if not st.session_state.processing_state['is_processing']:
                st.warning("–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                break
                
            end_idx = min(start_idx + chunk_size, total_rows)
            chunk = df.iloc[start_idx:end_idx]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            with progress_container:
                progress = end_idx / total_rows
                st.progress(progress)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
            with status_container:
                elapsed = time.time() - start_time
                rows_per_sec = end_idx / elapsed if elapsed > 0 else 0
                remaining_time = (total_rows - end_idx) / rows_per_sec if rows_per_sec > 0 else 0
                
                st.write(f"""
                **–ü—Ä–æ–≥—Ä–µ—Å—Å:** {end_idx}/{total_rows} —Å—Ç—Ä–æ–∫ ({progress:.1%})
                **–°–∫–æ—Ä–æ—Å—Ç—å:** {rows_per_sec:.1f} —Å—Ç—Ä–æ–∫/—Å–µ–∫
                **–û—Å—Ç–∞–ª–æ—Å—å:** {remaining_time:.0f} —Å–µ–∫
                """)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—É—â—É—é —á–∞—Å—Ç—å
            answers = chunk[selected_column].astype(str).tolist()
            chunk_grades = grader.predict_batch(answers)
            all_grades.extend(chunk_grades)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            with stats_container:
                if len(all_grades) > 0:
                    current_avg = np.mean(all_grades)
                    current_min = min(all_grades)
                    current_max = max(all_grades)
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞", f"{current_avg:.2f}")
                    with col2:
                        st.metric("–ú–∏–Ω. –æ—Ü–µ–Ω–∫–∞", f"{current_min:.2f}")
                    with col3:
                        st.metric("–ú–∞–∫—Å. –æ—Ü–µ–Ω–∫–∞", f"{current_max:.2f}")
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
            time.sleep(0.1)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è UI
        
        # –ó–∞–≤–µ—Ä—à–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
        st.session_state.processing_state['is_processing'] = False
        
        if len(all_grades) == total_rows:
            result_df = df.copy()
            result_df['predicted_grade'] = all_grades
            
            total_time = time.time() - start_time
            st.success(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(all_grades)} –æ—Ç–≤–µ—Ç–æ–≤ –∑–∞ {total_time:.1f} —Å–µ–∫")
            st.info(f"‚ö° –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {len(all_grades)/total_time:.1f} –æ—Ç–≤–µ—Ç–æ–≤/—Å–µ–∫")
            
            return result_df
        else:
            st.error(f"‚ùå –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ–ª—å–∫–æ {len(all_grades)} –∏–∑ {total_rows} —Å—Ç—Ä–æ–∫")
            return None
            
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
        st.session_state.processing_state['is_processing'] = False
        return None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
def load_grader():
    model_path = "my_trained_model_2"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—É—Ç–∏
    possible_paths = [
        model_path,
        f"./{model_path}",
        f"../{model_path}",
        f"../../{model_path}",
        "C:/Users/tkubanychbekov/Documents/Russian_exam_grader/my_trained_model_2"
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            st.info(f"üéØ –ù–∞–π–¥–µ–Ω–∞ –º–æ–¥–µ–ª—å –ø–æ –ø—É—Ç–∏: {path}")
            return RussianExamGrader(path)
    
    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
    st.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º –ø—É—Ç—è–º")
    st.info("""
    **–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ:**
    1. –ü–∞–ø–∫–∞ 'my_trained_model_2' –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —á—Ç–æ –∏ —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç
    2. –í –ø–∞–ø–∫–µ –µ—Å—Ç—å —Ñ–∞–π–ª—ã: pytorch_model.bin, config.json, tokenizer_config.json
    3. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞
    """)
    return None

# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
tab1, tab2, tab3 = st.tabs(["üéØ –û—Ü–µ–Ω–∏—Ç—å –æ–¥–∏–Ω –æ—Ç–≤–µ—Ç", "üìä –û—Ü–µ–Ω–∏—Ç—å —Ñ–∞–π–ª CSV", "üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã"])

with tab1:
    st.header("–û—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞")
    
    # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
    if not st.session_state.model_loaded:
        if st.button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏", key="load_model_single"):
            with st.spinner("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å..."):
                grader = load_grader()
                if grader is None:
                    st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")
                else:
                    st.session_state.grader_instance = grader
                    st.rerun()
    else:
        user_input = st.text_area(
            "–í–≤–µ–¥–∏—Ç–µ –æ—Ç–≤–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:",
            height=150,
            placeholder="–ù–∞–ø–∏—à–∏—Ç–µ –∑–¥–µ—Å—å –æ—Ç–≤–µ—Ç –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å...",
            key="single_answer"
        )

        if st.button("–û—Ü–µ–Ω–∏—Ç—å –æ—Ç–≤–µ—Ç", type="primary", key="single"):
            if user_input.strip():
                with st.spinner("ü§ñ –ú–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç..."):
                    start_time = time.time()
                    grade = st.session_state.grader_instance.predict(user_input)
                    processing_time = time.time() - start_time
                
                st.success(f"**–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {grade} / 5**")
                st.info(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.2f} —Å–µ–∫")
                
                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏
                col1, col2 = st.columns([1, 3])
                with col1:
                    st.metric("–û—Ü–µ–Ω–∫–∞", f"{grade}/5")
                with col2:
                    st.progress(grade / 5.0)
                
                # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏
                if grade >= 4.0:
                    st.info("üéâ –û—Ç–ª–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç!")
                elif grade >= 3.0:
                    st.info("üëç –•–æ—Ä–æ—à–∏–π –æ—Ç–≤–µ—Ç")
                elif grade >= 2.0:
                    st.warning("‚ö†Ô∏è –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç")
                else:
                    st.error("‚ùå –û—Ç–≤–µ—Ç —Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π")
            else:
                st.warning("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏.")

with tab2:
    st.header("–ü–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏–∑ CSV-—Ñ–∞–π–ª–∞")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å —Ç–µ–∫—É—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    if st.session_state.processing_state['is_processing']:
        st.warning("üîÑ –ò–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞...")
        if st.button("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É"):
            st.session_state.processing_state['is_processing'] = False
            st.rerun()
    
    st.markdown("""
    –ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV-—Ñ–∞–π–ª —Å –æ—Ç–≤–µ—Ç–∞–º–∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã (10,000+ —Å—Ç—Ä–æ–∫).
    **–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç:** UTF-8 —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º ';'
    """)
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å
    if not st.session_state.model_loaded:
        st.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        if st.button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏", key="load_model_batch"):
            with st.spinner("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å..."):
                grader = load_grader()
                if grader is not None:
                    st.session_state.grader_instance = grader
                    st.success("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
                    st.rerun()
    else:
        uploaded_file = st.file_uploader(
            "–í—ã–±–µ—Ä–∏—Ç–µ CSV-—Ñ–∞–π–ª", 
            type=['csv'],
            key="file_uploader"
        )
        
        if uploaded_file is not None and not st.session_state.processing_state['is_processing']:
            try:
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
                with st.spinner("üìñ –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª..."):
                    df = safe_read_csv(uploaded_file)
                
                st.subheader("üìä –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö")
                st.write(f"**–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:** {len(df)} —Å—Ç—Ä–æ–∫ √ó {len(df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫", len(df))
                with col2:
                    st.metric("–í—Å–µ–≥–æ —Å—Ç–æ–ª–±—Ü–æ–≤", len(df.columns))
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
                with st.expander("üëÄ –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫"):
                    st.dataframe(df.head(10))
                
                # –í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–∞ —Å –æ—Ç–≤–µ—Ç–∞–º–∏
                st.subheader("üéØ –í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–æ–ª–±–µ—Ü —Å –æ—Ç–≤–µ—Ç–∞–º–∏")
                selected_column = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–æ–ª–±–µ—Ü, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ç–µ–∫—Å—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤:",
                    df.columns,
                    index=0
                )
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                st.subheader("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                chunk_size = st.slider(
                    "–†–∞–∑–º–µ—Ä —á–∞—Å—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:",
                    min_value=100,
                    max_value=1000,
                    value=500,
                    step=100,
                    help="–ú–µ–Ω—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏, –Ω–æ –º–æ–≥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ"
                )
                
                if st.button("üöÄ –ù–∞—á–∞—Ç—å –æ—Ü–µ–Ω–∫—É –≤—Å–µ—Ö –æ—Ç–≤–µ—Ç–æ–≤", type="primary"):
                    if len(df) > 10000:
                        st.warning(f"‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª ({len(df)} —Å—Ç—Ä–æ–∫). –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç.")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                    st.session_state.processing_state.update({
                        'is_processing': True,
                        'total_rows': len(df),
                        'original_df': df,
                        'selected_column': selected_column
                    })
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º grader –∏–∑ session_state
                    result_df = process_large_dataset(
                        df, 
                        st.session_state.grader_instance, 
                        selected_column, 
                        chunk_size
                    )
                    
                    if result_df is not None:
                        st.session_state.graded_results = result_df
                        st.success("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–æ—Ç–æ–≤—ã! –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É '–†–µ–∑—É–ª—å—Ç–∞—Ç—ã'")
                        st.rerun()
                            
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —Ñ–∞–π–ª–æ–º: {e}")

with tab3:
    st.header("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏")
    
    if st.session_state.graded_results is not None:
        result_df = st.session_state.graded_results
        
        st.success(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ—Ç–≤–µ—Ç–æ–≤: {len(result_df)}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ü–µ–Ω–æ–∫")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            avg_grade = result_df['predicted_grade'].mean()
            st.metric("–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞", f"{avg_grade:.2f}")
        with col2:
            min_grade = result_df['predicted_grade'].min()
            st.metric("–ú–∏–Ω. –æ—Ü–µ–Ω–∫–∞", f"{min_grade:.2f}")
        with col3:
            max_grade = result_df['predicted_grade'].max()
            st.metric("–ú–∞–∫—Å. –æ—Ü–µ–Ω–∫–∞", f"{max_grade:.2f}")
        with col4:
            st.metric("–í—Å–µ–≥–æ –æ—Ç–≤–µ—Ç–æ–≤", len(result_df))
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫
        st.subheader("üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫")
        
        # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞
        grade_bins = pd.cut(result_df['predicted_grade'], 
                           bins=[0, 1, 2, 3, 4, 5], 
                           labels=['0-1', '1-2', '2-3', '3-4', '4-5'])
        grade_distribution = grade_bins.value_counts().sort_index()
        st.bar_chart(grade_distribution)
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
        st.subheader("üìã –î–µ—Ç–∞–ª–∏ –æ—Ü–µ–Ω–æ–∫")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
        page_size = 100
        total_pages = max(1, len(result_df) // page_size)
        
        page = st.number_input("–°—Ç—Ä–∞–Ω–∏—Ü–∞", min_value=1, max_value=total_pages, value=1)
        
        start_idx = (page - 1) * page_size
        end_idx = min(start_idx + page_size, len(result_df))
        
        selected_column = st.session_state.processing_state.get('selected_column', 'answer')
        
        st.dataframe(
            result_df.iloc[start_idx:end_idx][
                [selected_column, 'predicted_grade']
            ],
            height=400
        )
        
        # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        st.subheader("üíæ –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        
        csv_data = result_df.to_csv(index=False, sep=';').encode('utf-8')
        st.download_button(
            label=f"üì• –°–∫–∞—á–∞—Ç—å –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ({len(result_df)} —Å—Ç—Ä–æ–∫)",
            data=csv_data,
            file_name=f"graded_answers_{len(result_df)}_rows.csv",
            mime="text/csv"
        )
        
    else:
        st.info("‚ÑπÔ∏è –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –ø–æ—è–≤—è—Ç—Å—è –∑–¥–µ—Å—å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞")

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
with st.sidebar:
    st.header("‚ÑπÔ∏è –û —Å–∏—Å—Ç–µ–º–µ")
    st.markdown("""
    **–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:**
    - –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DeepPavlov
    - –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
    - –§–æ—Ä–º–∞—Ç: CSV (UTF-8, ;)
    """)
    
    st.header("üìä –°—Ç–∞—Ç—É—Å")
    if st.session_state.model_loaded:
        st.success("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    else:
        st.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    if st.session_state.processing_state['is_processing']:
        st.warning("–ò–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∞")
        progress = st.session_state.processing_state.get('current_index', 0) / max(1, st.session_state.processing_state.get('total_rows', 1))
        st.progress(progress)
    else:
        st.success("–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
    
    if st.session_state.graded_results is not None:
        st.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(st.session_state.graded_results)} –æ—Ç–≤–µ—Ç–æ–≤")

# –§—É—Ç–µ—Ä
st.markdown("---")
st.markdown("**–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ —ç–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤** ‚Ä¢ –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DeepPavlov ‚Ä¢ –§–æ—Ä–º–∞—Ç: CSV (UTF-8, ;)")
