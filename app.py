import streamlit as st
import pandas as pd
import numpy as np
import os
import time
import re
import sys

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="Russian Exam Grader",
    page_icon="üá∑üá∫",
    layout="wide"
)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.title("üá∑üá∫ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞ –ø–æ —Ä—É—Å—Å–∫–æ–º—É —è–∑—ã–∫—É")
st.markdown("""
**‚ö° –í–µ—Ä—Å–∏—è —Å ML –º–æ–¥–µ–ª—å—é –∏–∑ Git LFS**  
–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV-—Ñ–∞–π–ª —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è–º–∏ –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏.
""")

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
def safe_read_csv(uploaded_file):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ CSV"""
    for encoding in ['utf-8', 'cp1251', 'windows-1251']:
        for sep in [';', ',', '\t']:
            try:
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, encoding=encoding, sep=sep)
                if len(df.columns) > 1:
                    return df
            except:
                continue
    
    try:
        uploaded_file.seek(0)
        return pd.read_csv(uploaded_file, on_bad_lines='skip')
    except:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª")

def clean_html_simple(html_text):
    """–ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ HTML"""
    if pd.isna(html_text): 
        return ""
    text = re.sub(r'<[^>]+>', '', str(html_text))
    text = re.sub(r'\s{2,}', ' ', text)
    return text.strip()

def check_model_files(model_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏"""
    required_files = [
        'pytorch_model.bin',
        'config.json', 
        'tokenizer_config.json',
        'vocab.txt'
    ]
    
    existing_files = []
    missing_files = []
    
    for file in required_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            existing_files.append(file)
        else:
            missing_files.append(file)
    
    return existing_files, missing_files

# –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—å—é
class RussianExamGrader:
    def __init__(self, model_path="my_trained_model_2"):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.device = "CPU"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏
        existing_files, missing_files = check_model_files(model_path)
        
        if existing_files:
            st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏: {', '.join(existing_files)}")
        
        if missing_files:
            st.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã: {', '.join(missing_files)}")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã
        if 'pytorch_model.bin' in existing_files and 'config.json' in existing_files:
            try:
                self._load_model()
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                st.info("üí° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ–º–æ-—Ä–µ–∂–∏–º")
        else:
            st.info("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ–º–æ-—Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏")
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ ML –º–æ–¥–µ–ª–∏"""
        try:
            import torch
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            
            st.info("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
                st.success(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {torch.cuda.get_device_name()}")
            else:
                self.device = torch.device('cpu')
                st.info("üíª –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)
            self.model.to(self.device)
            self.model.eval()
            
            st.success("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            
        except ImportError as e:
            st.error(f"‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã ML –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {e}")
            st.info("üí° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ–º–æ-—Ä–µ–∂–∏–º")
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            st.info("üí° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ–º–æ-—Ä–µ–∂–∏–º")
    
    def predict_single_fast(self, question_text, transcription_text, question_number):
        """–û—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        if self.model is None:
            # –î–µ–º–æ-—Ä–µ–∂–∏–º
            return self._demo_predict_single(question_text, transcription_text, question_number)
        else:
            # –†–µ–∂–∏–º —Å ML –º–æ–¥–µ–ª—å—é
            return self._ml_predict_single(question_text, transcription_text, question_number)
    
    def _ml_predict_single(self, question_text, transcription_text, question_number):
        """ML –æ—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        try:
            import torch
            
            # –û—á–∏—Å—Ç–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞
            cleaned_question = clean_html_simple(question_text)
            input_text = f"–ó–ê–î–ê–ù–ò–ï: {cleaned_question} | –î–ò–ê–õ–û–ì: {transcription_text}"
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(
                input_text,
                max_length=512,
                padding=True,
                truncation=True,
                return_tensors='pt'
            ).to(self.device)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                outputs = self.model(**inputs)
                raw_score = float(outputs.logits.cpu().numpy()[0][0])
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
            max_score = {1: 1.0, 2: 2.0, 3: 1.0, 4: 2.0}.get(question_number, 2.0)
            final_score = int(round(np.clip(raw_score, 0, max_score)))
            
            return final_score, raw_score
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ ML –æ—Ü–µ–Ω–∫–∏: {e}")
            return self._demo_predict_single(question_text, transcription_text, question_number)
    
    def _demo_predict_single(self, question_text, transcription_text, question_number):
        """–î–µ–º–æ-–æ—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        time.sleep(0.05)
        
        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –¥–µ–º–æ-–æ—Ü–µ–Ω–∫–∞
        text_length = len(transcription_text)
        word_count = len(transcription_text.split())
        base_score = min(2.0, word_count / 20)
        random_factor = np.random.normal(0, 0.3)
        raw_score = max(0, min(2.0, base_score + random_factor))
        
        max_score = {1: 1.0, 2: 2.0, 3: 1.0, 4: 2.0}.get(question_number, 2.0)
        final_score = int(round(np.clip(raw_score, 0, max_score)))
        
        return final_score, float(raw_score)
    
    def predict_batch_gpu_optimized(self, df, batch_size=100, max_length=384):
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞"""
        if self.model is None:
            return self._demo_predict_batch(df, batch_size)
        else:
            return self._ml_predict_batch(df, batch_size, max_length)
    
    def _ml_predict_batch(self, df, batch_size, max_length):
        """ML –ø–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞"""
        try:
            import torch
            
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
            df_copy = df.copy()
            mask = ~(df_copy['–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞'].isna() | df_copy['–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞'].isna())
            df_copy = df_copy[mask].copy()
            
            df_copy['–¢–µ–∫—Å—Ç_–æ—á–∏—â–µ–Ω–Ω—ã–π'] = df_copy['–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞'].apply(clean_html_simple)
            df_copy['Input_Text'] = "–ó–ê–î–ê–ù–ò–ï: " + df_copy['–¢–µ–∫—Å—Ç_–æ—á–∏—â–µ–Ω–Ω—ã–π'] + " | –î–ò–ê–õ–û–ì: " + df_copy['–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞']
            
            texts = df_copy['Input_Text'].tolist()
            question_numbers = df_copy['‚Ññ –≤–æ–ø—Ä–æ—Å–∞'].values
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            all_predictions = []
            total_samples = len(texts)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏
            for i in range(0, total_samples, batch_size):
                batch_texts = texts[i:i + batch_size]
                current_batch_size = len(batch_texts)
                
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–∞
                inputs = self.tokenizer(
                    batch_texts,
                    max_length=max_length,
                    padding=True,
                    truncation=True,
                    return_tensors="pt"
                ).to(self.device)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    batch_predictions = outputs.logits.squeeze()
                
                if batch_predictions.ndim == 0:
                    batch_predictions = [float(batch_predictions.cpu().numpy())]
                elif batch_predictions.ndim == 1:
                    batch_predictions = batch_predictions.cpu().numpy().tolist()
                else:
                    batch_predictions = batch_predictions.cpu().numpy().flatten().tolist()
                
                all_predictions.extend(batch_predictions[:current_batch_size])
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                progress = min((i + current_batch_size) / total_samples, 1.0)
                progress_bar.progress(progress)
                status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {min(i + current_batch_size, total_samples)}/{total_samples}")
            
            progress_bar.empty()
            status_text.empty()
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
            result_df = df.iloc[df_copy.index].copy() if len(df_copy) < len(df) else df.copy()
            result_df['predicted_score'] = all_predictions
            
            def finalize_score(row):
                score = row['predicted_score']
                question_num = row['‚Ññ –≤–æ–ø—Ä–æ—Å–∞']
                max_score = {1: 1.0, 2: 2.0, 3: 1.0, 4: 2.0}.get(question_num, 2.0)
                return int(round(np.clip(score, 0, max_score)))
            
            result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted'] = result_df.apply(finalize_score, axis=1)
            
            st.success(f"‚úÖ ML –æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_samples} –æ—Ç–≤–µ—Ç–æ–≤")
            return result_df.drop(columns=['predicted_score'], errors='ignore')
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ ML –ø–∞–∫–µ—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏: {e}")
            return self._demo_predict_batch(df, batch_size)
    
    def _demo_predict_batch(self, df, batch_size):
        """–î–µ–º–æ –ø–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞"""
        try:
            # –î–µ–º–æ-–æ–±—Ä–∞–±–æ—Ç–∫–∞
            result_df = df.copy()
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            total_rows = len(result_df)
            
            for i in range(total_rows):
                # –î–µ–º–æ-–æ—Ü–µ–Ω–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
                transcription = str(result_df.iloc[i]['–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞'])
                word_count = len(transcription.split())
                base_score = min(2.0, word_count / 25)
                random_factor = np.random.normal(0, 0.2)
                raw_score = max(0, min(2.0, base_score + random_factor))
                
                question_num = result_df.iloc[i]['‚Ññ –≤–æ–ø—Ä–æ—Å–∞']
                max_score = {1: 1.0, 2: 2.0, 3: 1.0, 4: 2.0}.get(question_num, 2.0)
                final_score = int(round(np.clip(raw_score, 0, max_score)))
                
                result_df.loc[result_df.index[i], '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted'] = final_score
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                if i % 10 == 0 or i == total_rows - 1:
                    progress = (i + 1) / total_rows
                    progress_bar.progress(progress)
                    status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i+1}/{total_rows} –æ—Ç–≤–µ—Ç–æ–≤")
            
            progress_bar.empty()
            status_text.empty()
            
            st.success(f"‚úÖ –î–µ–º–æ-–æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_rows} –æ—Ç–≤–µ—Ç–æ–≤")
            return result_df
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –¥–µ–º–æ-–æ—Ü–µ–Ω–∫–∏: {e}")
            return None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–µ—Ä–∞
@st.cache_resource
def load_grader():
    return RussianExamGrader()

# –ó–∞–≥—Ä—É–∂–∞–µ–º –≥—Ä–∞–¥–µ—Ä
grader = load_grader()

# –î–∞–ª—å—à–µ —Ç–æ—Ç –∂–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —á—Ç–æ –∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –∫–æ–¥–µ...
# [–û–°–¢–ê–í–¨–¢–ï –í–ï–°–¨ –ò–ù–¢–ï–†–§–ï–ô–° –ò–ó –ü–†–ï–î–´–î–£–©–ï–ì–û –ö–û–î–ê –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô]

# –°–æ–∑–¥–∞–µ–º –≤–∫–ª–∞–¥–∫–∏
tab1, tab2 = st.tabs(["üéØ –û—Ü–µ–Ω–∏—Ç—å –æ–¥–∏–Ω –æ—Ç–≤–µ—Ç", "üìä –û—Ü–µ–Ω–∏—Ç—å —Ñ–∞–π–ª CSV"])

with tab1:
    st.header("–û—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞")
    
    col1, col2 = st.columns(2)
    with col1:
        question_number = st.selectbox("‚Ññ –≤–æ–ø—Ä–æ—Å–∞:", [1, 2, 3, 4], key="question_number")
    with col2:
        max_score = {1: 1, 2: 2, 3: 1, 4: 2}.get(question_number, 2)
        st.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª: {max_score}")
    
    question_text = st.text_area(
        "–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞:",
        height=100,
        placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞...",
        key="question_text"
    )
    
    transcription_text = st.text_area(
        "–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞:",
        height=150,
        placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–∞...",
        key="transcription_text"
    )

    if st.button("‚ö° –û—Ü–µ–Ω–∏—Ç—å –æ—Ç–≤–µ—Ç", type="primary", key="single"):
        if question_text.strip() and transcription_text.strip():
            with st.spinner("ü§ñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç..."):
                start_time = time.time()
                try:
                    final_score, raw_score = grader.predict_single_fast(question_text, transcription_text, question_number)
                    processing_time = time.time() - start_time
                    
                    mode = "ML –º–æ–¥–µ–ª—å" if grader.model is not None else "–î–µ–º–æ-—Ä–µ–∂–∏–º"
                    st.success(f"**–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {final_score} / {max_score}** ({mode})")
                    
                    col1, col2 = st.columns([1, 3])
                    with col1:
                        st.metric("–û—Ü–µ–Ω–∫–∞", f"{final_score}/{max_score}")
                    with col2:
                        st.progress(final_score / max_score)
                    
                    with st.expander("üîç –î–µ—Ç–∞–ª–∏ –∞–Ω–∞–ª–∏–∑–∞"):
                        st.write(f"**–°—ã—Ä–∞—è –æ—Ü–µ–Ω–∫–∞:** {raw_score:.4f}")
                        st.write(f"**–†–µ–∂–∏–º:** {mode}")
                        st.write(f"**–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:** {processing_time:.3f} —Å–µ–∫")
                        if grader.model is None:
                            st.info("üí° –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ML –º–æ–¥–µ–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: torch, transformers")
                        
                except Exception as e:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ: {e}")
        else:
            st.warning("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–æ–ª—è.")

with tab2:
    st.header("–ü–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏–∑ CSV-—Ñ–∞–π–ª–∞")
    
    st.markdown("""
    **–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –≤–µ—Ä—Å–∏–∏:**
    - üöÄ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞ (ML/–î–µ–º–æ)
    - üéØ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    - üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    - ‚ö° –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
    """)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    st.subheader("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    col1, col2 = st.columns(2)
    with col1:
        batch_size = st.slider(
            "–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞:",
            min_value=50,
            max_value=200,
            value=100,
            help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –≤ –ø–∞–∫–µ—Ç–µ"
        )
    with col2:
        mode = "ML –º–æ–¥–µ–ª—å" if grader.model is not None else "–î–µ–º–æ-—Ä–µ–∂–∏–º"
        st.metric("–†–µ–∂–∏–º", mode)
        if grader.model is None:
            st.info("üí° –î–µ–º–æ-—Ä–µ–∂–∏–º: –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞")
    
    uploaded_file = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ CSV-—Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ —ç–∫–∑–∞–º–µ–Ω–∞", 
        type=['csv', 'txt'],
        key="file_uploader"
    )
    
    if uploaded_file is not None:
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
            with st.spinner("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª..."):
                df = safe_read_csv(uploaded_file)
            
            st.success(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            required_columns = ['‚Ññ –≤–æ–ø—Ä–æ—Å–∞', '–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞', '–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                st.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing_columns)}")
                st.info(f"üìã –ù–∞–π–¥–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏: {', '.join(df.columns)}")
            else:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö
                st.subheader("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º:**")
                    question_stats = df['‚Ññ –≤–æ–ø—Ä–æ—Å–∞'].value_counts().sort_index()
                    for q_num, count in question_stats.items():
                        max_score = {1: 1, 2: 2, 3: 1, 4: 2}.get(q_num, 2)
                        st.write(f"- –í–æ–ø—Ä–æ—Å {q_num}: {count} –æ—Ç–≤–µ—Ç–æ–≤ (–º–∞–∫—Å. {max_score} –±–∞–ª–ª–æ–≤)")
                
                with col2:
                    st.write("**–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:**")
                    display_cols = ['‚Ññ –≤–æ–ø—Ä–æ—Å–∞', '–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞', '–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞']
                    if '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞' in df.columns:
                        display_cols.append('–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞')
                    st.dataframe(df[display_cols].head(3))
                
                if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ—Ü–µ–Ω–∫—É", type="primary", key="batch"):
                    with st.spinner("‚ö° –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ..."):
                        try:
                            result_df = grader.predict_batch_gpu_optimized(df, batch_size=batch_size)
                            
                            if result_df is not None:
                                st.balloons()
                                st.subheader("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏")
                                
                                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                                display_columns = ['‚Ññ –≤–æ–ø—Ä–æ—Å–∞', '–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞', '–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞', '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted']
                                if '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞' in result_df.columns:
                                    display_columns.insert(3, '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞')
                                
                                st.dataframe(result_df[display_columns].head(10))
                                
                                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                                st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ü–µ–Ω–æ–∫")
                                
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    avg_grade = result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted'].mean()
                                    st.metric("–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞", f"{avg_grade:.2f}")
                                with col2:
                                    min_grade = result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted'].min()
                                    st.metric("–ú–∏–Ω. –æ—Ü–µ–Ω–∫–∞", f"{min_grade}")
                                with col3:
                                    max_grade = result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted'].max()
                                    st.metric("–ú–∞–∫—Å. –æ—Ü–µ–Ω–∫–∞", f"{max_grade}")
                                with col4:
                                    total_count = len(result_df)
                                    st.metric("–í—Å–µ–≥–æ –æ—Ç–≤–µ—Ç–æ–≤", total_count)
                                
                                # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                                st.subheader("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫")
                                grade_counts = result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted'].value_counts().sort_index()
                                st.bar_chart(grade_counts)
                                
                                # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ
                                st.subheader("üíæ –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
                                csv_data = result_df.to_csv(index=False, sep=';').encode('utf-8')
                                st.download_button(
                                    label="üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (CSV)",
                                    data=csv_data,
                                    file_name="graded_results.csv",
                                    mime="text/csv",
                                    key="download_full"
                                )
                                
                        except Exception as e:
                            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
                            
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {e}")

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
with st.sidebar:
    st.header("‚ö° –û —Å–∏—Å—Ç–µ–º–µ")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    st.subheader("üì¶ –ú–æ–¥–µ–ª—å")
    existing_files, missing_files = check_model_files("my_trained_model_2")
    
    if existing_files:
        st.success(f"–§–∞–π–ª—ã: {len(existing_files)}/{len(existing_files) + len(missing_files)}")
        for file in existing_files:
            st.write(f"‚úÖ {file}")
    
    if missing_files:
        st.warning("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç:")
        for file in missing_files:
            st.write(f"‚ùå {file}")
    
    st.subheader("üéØ –†–µ–∂–∏–º")
    if grader.model is not None:
        st.success("ML –º–æ–¥–µ–ª—å –∞–∫—Ç–∏–≤–Ω–∞")
        st.info(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {grader.device}")
    else:
        st.info("–î–µ–º–æ-—Ä–µ–∂–∏–º")
        st.write("–î–ª—è ML –º–æ–¥–µ–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ:")
        st.code("pip install torch transformers")

# –§—É—Ç–µ—Ä
st.markdown("---")
st.markdown(
    "**–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ —ç–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤** ‚Ä¢ "
    "–ü–æ–¥–¥–µ—Ä–∂–∫–∞ Git LFS ‚Ä¢ "
    "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º ML/–î–µ–º–æ"
)
