import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import pandas as pd
import numpy as np
import re
import streamlit as st
from typing import List, Optional
import time

class OptimizedRussianExamGrader:
    def __init__(self, model_path, batch_size=32, use_fp16=True):
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.
        
        Args:
            model_path (str): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –º–æ–¥–µ–ª—å—é
            batch_size (int): –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            use_fp16 (bool): –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ FP16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (—Ç–æ–ª—å–∫–æ –¥–ª—è GPU)
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.batch_size = batch_size
        self.use_fp16 = use_fp16 and self.device.type == 'cuda'
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ PyTorch
        if self.device.type == 'cuda':
            torch.backends.cudnn.benchmark = True
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        
        # –ü–µ—Ä–µ–Ω–æ—Å –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.model.to(self.device)
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ FP16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ GPU
        if self.use_fp16:
            self.model.half()
            st.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è FP16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ GPU")
        
        self.model.eval()
        
        st.success(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        st.info(f"üìä –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")

    def preprocess_text(self, text):
        """
        –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞.
        """
        text = str(text).lower()
        text = re.sub(r'[^\w\s]', '', text)
        return text

    def predict_single(self, text):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
        """
        processed_text = self.preprocess_text(text)
        inputs = self.tokenizer(
            processed_text,
            max_length=512,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        ).to(self.device)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ fp16 –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        if self.use_fp16:
            inputs = {k: v.half() if v.dtype == torch.float32 else v 
                     for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model(**inputs)
            prediction = outputs.logits.cpu().numpy()

        grade = float(prediction[0][0])
        grade = max(0, min(5, grade))
        return round(grade, 2)

    def predict_batch(self, texts: List[str]) -> List[float]:
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤.
        
        Args:
            texts (List[str]): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            List[float]: –°–ø–∏—Å–æ–∫ –æ—Ü–µ–Ω–æ–∫ –æ—Ç 0 –¥–æ 5
        """
        if not texts:
            return []
        
        try:
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
            processed_texts = [self.preprocess_text(text) for text in texts]
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–∞
            inputs = self.tokenizer(
                processed_texts,
                max_length=512,
                padding=True,  # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–∞–¥–¥–∏–Ω–≥
                truncation=True,
                return_tensors='pt'
            ).to(self.device)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ fp16 –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
            if self.use_fp16:
                inputs = {k: v.half() if v.dtype == torch.float32 else v 
                         for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = outputs.logits.cpu().numpy()

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            grades = predictions[:, 0].tolist()
            grades = [max(0, min(5, float(grade))) for grade in grades]
            return [round(grade, 2) for grade in grades]
            
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return [0.0] * len(texts)

    def predict_large_dataset(self, texts: List[str], 
                            progress_callback: Optional[callable] = None) -> List[float]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Ç–µ–∫—Å—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏.
        
        Args:
            texts (List[str]): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            progress_callback (callable): –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            
        Returns:
            List[float]: –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –æ—Ü–µ–Ω–æ–∫
        """
        if not texts:
            return []
        
        all_grades = []
        total_batches = (len(texts) + self.batch_size - 1) // self.batch_size
        
        st.info(f"üî¢ –í—Å–µ–≥–æ –±–∞—Ç—á–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_batches}")
        
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]
            batch_grades = self.predict_batch(batch_texts)
            all_grades.extend(batch_grades)
            
            # –í—ã–∑–æ–≤ callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if progress_callback:
                progress = (i + len(batch_texts)) / len(texts)
                processed_count = i + len(batch_texts)
                progress_callback(progress, processed_count, len(texts))
        
        return all_grades

    def benchmark_performance(self, sample_texts: List[str], num_runs: int = 3):
        """
        –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.
        
        Args:
            sample_texts (List[str]): –ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            num_runs (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—É—Å–∫–æ–≤ –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è
        """
        st.header("üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        
        times = []
        for run in range(num_runs):
            start_time = time.time()
            _ = self.predict_large_dataset(sample_texts)
            end_time = time.time()
            times.append(end_time - start_time)
        
        avg_time = np.mean(times)
        speed = len(sample_texts) / avg_time
        
        st.metric("–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏", f"{avg_time:.2f} —Å–µ–∫")
        st.metric("–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏", f"{speed:.2f} –æ—Ç–≤–µ—Ç–æ–≤/—Å–µ–∫")
        st.metric("–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞", self.batch_size)
        st.metric("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", str(self.device))

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ CSV
def process_csv_file(csv_path: str, grader: OptimizedRussianExamGrader, 
                    text_column: str = 'answer') -> pd.DataFrame:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ CSV —Ñ–∞–π–ª–∞ —Å –æ—Ç–≤–µ—Ç–∞–º–∏.
    
    Args:
        csv_path (str): –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
        grader (OptimizedRussianExamGrader): –û–±—ä–µ–∫—Ç –æ—Ü–µ–Ω—â–∏–∫–∞
        text_column (str): –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤
        
    Returns:
        pd.DataFrame: DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏
    """
    try:
        # –ß—Ç–µ–Ω–∏–µ CSV
        df = pd.read_csv(csv_path, encoding='utf-8')
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω—É–∂–Ω–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞
        if text_column not in df.columns:
            available_columns = list(df.columns)
            raise ValueError(f"–°—Ç–æ–ª–±–µ—Ü '{text_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {available_columns}")
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤
        answers = df[text_column].astype(str).tolist()
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è Streamlit
        progress_bar = st.progress(0)
        status_text = st.empty()
        time_text = st.empty()
        
        start
