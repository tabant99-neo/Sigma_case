import streamlit as st
import pandas as pd
import numpy as np
import re
import torch
import re
import tempfile
import os
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import time
from contextlib import contextmanager

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="Russian Exam Grader - GPU –£—Å–∫–æ—Ä–µ–Ω–∏–µ",
    page_icon="üá∑üá∫",
    layout="wide"
)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.title("üá∑üá∫ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞ –ø–æ —Ä—É—Å—Å–∫–æ–º—É —è–∑—ã–∫—É")
st.markdown("""
**‚ö° GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–æ–π**  
–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV-—Ñ–∞–π–ª —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è–º–∏ –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏.
""")

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
MODEL_NAME = "DeepPavlov/rubert-base-cased"

# --- –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –ü–†–ï–û–ë–†–ê–ë–û–¢–ö–ò ---

def clean_html(html_text):
    """–ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ HTML –±–µ–∑ BeautifulSoup"""
    if pd.isna(html_text): 
        return ""
    
    # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–º–µ–Ω–∞ HTML —Ç–µ–≥–æ–≤ —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
    text = re.sub(r'<[^>]+>', '', str(html_text))  # –£–¥–∞–ª—è–µ–º –≤—Å–µ HTML —Ç–µ–≥–∏
    text = re.sub(r'&nbsp;', ' ', text)  # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'&amp;', '&', text)   # –ó–∞–º–µ–Ω—è–µ–º HTML entities
    text = re.sub(r'&lt;', '<', text)
    text = re.sub(r'&gt;', '>', text)
    text = re.sub(r'&quot;', '"', text)
    text = re.sub(r'‚Äì\s*', '', text)
    text = re.sub(r'\s{2,}', ' ', text)  # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    return text.strip()

def normalize_score(score_series):
    return score_series.astype(float)

def preprocess_data_fast(df):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"""
    df_copy = df.copy()
    
    # –ë—ã—Å—Ç—Ä–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
    mask = ~(df_copy['–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞'].isna() | df_copy['–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞'].isna())
    if '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞' in df_copy.columns:
        mask &= ~df_copy['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].isna()
    
    df_copy = df_copy[mask].copy()
    
    # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ HTML
    df_copy['–¢–µ–∫—Å—Ç_–≤–æ–ø—Ä–æ—Å–∞_–æ—á–∏—â–µ–Ω–Ω—ã–π'] = df_copy['–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞'].apply(clean_html)
    
    # –ë—ã—Å—Ç—Ä–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ Input_Text
    df_copy['Input_Text'] = "–ó–ê–î–ê–ù–ò–ï: " + df_copy['–¢–µ–∫—Å—Ç_–≤–æ–ø—Ä–æ—Å–∞_–æ—á–∏—â–µ–Ω–Ω—ã–π'] + \
                           " | –î–ò–ê–õ–û–ì: " + df_copy['–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞']
    
    if '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞' in df_copy.columns and not df_copy['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].isnull().all():
        df_copy['labels'] = normalize_score(df_copy['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'])
    else:
        df_copy['labels'] = np.nan
        
    return df_copy

def finalize_score_vectorized(scores, question_numbers):
    """–í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ü–µ–Ω–æ–∫"""
    max_scores = np.array([{1: 1.0, 2: 2.0, 3: 1.0, 4: 2.0}.get(q, 2.0) for q in question_numbers])
    clipped_scores = np.clip(scores, 0.0, max_scores)
    final_scores = np.round(clipped_scores).astype(int)
    return np.clip(final_scores, 0, max_scores.astype(int))

# --- GPU-–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ö–õ–ê–°–° –î–õ–Ø –û–¶–ï–ù–ö–ò ---

class RussianExamGraderGPU:
    def __init__(self, model_path):
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º GPU —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è GPU
        if self.device.type == 'cuda':
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        
        try:
            st.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ {self.device}...")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è GPU
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è GPU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏ —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            torch_dtype = torch.float16 if self.device.type == 'cuda' else torch.float32
            
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_path, 
                torch_dtype=torch_dtype
            )
            
            # –ü–µ—Ä–µ–Ω–æ—Å –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.model.to(self.device)
            self.model.eval()
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            if self.device.type == 'cuda':
                self.model = torch.compile(self.model)  # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è
            
            st.success(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {self.device}!")
            if self.device.type == 'cuda':
                st.info(f"üéØ GPU: {torch.cuda.get_device_name()}, –ü–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            raise e

    @contextmanager
    def inference_mode(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ inference"""
        original_grad = torch.is_grad_enabled()
        try:
            torch.set_grad_enabled(False)
            yield
        finally:
            torch.set_grad_enabled(original_grad)

    def predict_batch_gpu_optimized(self, df, batch_size=128, max_length=384):
        """
        GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞
        –£—Å–∫–æ—Ä–µ–Ω–∏–µ 10-20x –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        """
        try:
            with self.inference_mode():
                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
                st.info("üîß –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
                start_time = time.time()
                df_processed = preprocess_data_fast(df.copy())
                texts = df_processed['Input_Text'].tolist()
                question_numbers = df_processed['‚Ññ –≤–æ–ø—Ä–æ—Å–∞'].values
                
                preprocessing_time = time.time() - start_time
                st.info(f"‚è±Ô∏è –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–Ω—è–ª–∞: {preprocessing_time:.2f} —Å–µ–∫")
                
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
                progress_bar = st.progress(0)
                status_text = st.empty()
                speed_text = st.empty()
                
                all_predictions = []
                total_samples = len(texts)
                
                st.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º GPU-—É—Å–∫–æ—Ä–µ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {total_samples} –æ—Ç–≤–µ—Ç–æ–≤...")
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏
                for i in range(0, total_samples, batch_size):
                    batch_start = time.time()
                    batch_texts = texts[i:i + batch_size]
                    current_batch_size = len(batch_texts)
                    
                    # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–∞
                    inputs = self.tokenizer(
                        batch_texts,
                        max_length=max_length,  # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                        padding=True,  # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–∞–¥–¥–∏–Ω–≥
                        truncation=True,
                        return_tensors="pt"
                    ).to(self.device, non_blocking=True)
                    
                    # –ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ GPU
                    with torch.cuda.amp.autocast(enabled=self.device.type == 'cuda'):
                        outputs = self.model(**inputs)
                        batch_predictions = outputs.logits.squeeze()
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ª—É—á–∞–∏ dimensionalities
                    if batch_predictions.ndim == 0:  # –û–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç
                        batch_predictions = [float(batch_predictions.cpu().numpy())]
                    elif batch_predictions.ndim == 1:  # –û–¥–∏–Ω –±–∞—Ç—á
                        batch_predictions = batch_predictions.cpu().numpy().tolist()
                    else:  # –ù–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–º–µ—Ä–µ–Ω–∏–π
                        batch_predictions = batch_predictions.cpu().numpy().flatten().tolist()
                    
                    all_predictions.extend(batch_predictions[:current_batch_size])
                    
                    # –†–∞—Å—á–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏
                    batch_time = time.time() - batch_start
                    samples_per_second = current_batch_size / batch_time if batch_time > 0 else 0
                    
                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                    progress = min((i + current_batch_size) / total_samples, 1.0)
                    progress_bar.progress(progress)
                    
                    status_text.text(
                        f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {min(i + current_batch_size, total_samples)}/{total_samples} "
                        f"(–±–∞—Ç—á: {current_batch_size})"
                    )
                    
                    speed_text.text(
                        f"‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {samples_per_second:.1f} –æ—Ç–≤–µ—Ç–æ–≤/—Å–µ–∫ | "
                        f"–û—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è: {(total_samples - i - current_batch_size) / samples_per_second / 60:.1f} –º–∏–Ω"
                    )
                
                # –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–≤
                progress_bar.empty()
                status_text.empty()
                speed_text.empty()
                
                # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
                st.info("üîß –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ü–µ–Ω–æ–∫...")
                final_predictions = finalize_score_vectorized(
                    np.array(all_predictions), 
                    question_numbers
                )
                
                # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                df_result = df.iloc[df_processed.index].copy() if len(df_processed) < len(df) else df.copy()
                df_result['predicted_score'] = all_predictions
                df_result['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted'] = final_predictions
                
                total_time = time.time() - start_time
                st.success(f"‚úÖ –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.2f} —Å–µ–∫! "
                          f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_samples} –æ—Ç–≤–µ—Ç–æ–≤ "
                          f"({total_samples/total_time:.1f} –æ—Ç–≤–µ—Ç–æ–≤/—Å–µ–∫)")
                
                return df_result.drop(columns=['predicted_score'], errors='ignore')
                
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ GPU-–æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
            import traceback
            st.code(traceback.format_exc())
            return None

    def predict_single_fast(self, question_text, transcription_text, question_number):
        """–ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        try:
            with self.inference_mode():
                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
                cleaned_question = clean_html(question_text)
                input_text = f"–ó–ê–î–ê–ù–ò–ï: {cleaned_question} | –î–ò–ê–õ–û–ì: {transcription_text}"
                
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
                inputs = self.tokenizer(
                    input_text,
                    max_length=384,
                    padding=True,
                    truncation=True,
                    return_tensors='pt'
                ).to(self.device, non_blocking=True)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                with torch.cuda.amp.autocast(enabled=self.device.type == 'cuda'):
                    outputs = self.model(**inputs)
                    raw_score = float(outputs.logits.cpu().numpy()[0][0])
                
                # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
                max_score = {1: 1.0, 2: 2.0, 3: 1.0, 4: 2.0}.get(question_number, 2.0)
                clipped_score = np.clip(raw_score, 0.0, max_score)
                final_score = int(round(clipped_score))
                
                return final_score, raw_score
                
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
            return 0, 0.0

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---

def safe_read_csv(uploaded_file):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ CSV"""
    encodings = ['utf-8', 'cp1251', 'windows-1251', 'iso-8859-1']
    
    for encoding in encodings:
        try:
            uploaded_file.seek(0)
            for sep in [',', ';', '\t']:
                try:
                    uploaded_file.seek(0)
                    df = pd.read_csv(uploaded_file, encoding=encoding, sep=sep)
                    if len(df.columns) > 0:
                        return df
                except:
                    continue
        except UnicodeDecodeError:
            continue
        except Exception:
            continue
    
    try:
        uploaded_file.seek(0)
        return pd.read_csv(uploaded_file, sep='\t', encoding='utf-8')
    except:
        pass
    
    try:
        uploaded_file.seek(0)
        return pd.read_csv(uploaded_file, encoding='utf-8', on_bad_lines='skip')
    except:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª")

# --- –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò ---

@st.cache_resource
def load_grader_gpu():
    model_path = "my_trained_model_2"
    
    if not os.path.exists(model_path):
        absolute_path = "C:/Users/tkubanychbekov/Documents/Russian_exam_grader/my_trained_model_2"
        if os.path.exists(absolute_path):
            model_path = absolute_path
        else:
            st.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {model_path}")
    
    return RussianExamGraderGPU(model_path)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
try:
    grader = load_grader_gpu()
except Exception as e:
    st.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
    st.stop()

# --- –ò–ù–¢–ï–†–§–ï–ô–° STREAMLIT ---

# –°–æ–∑–¥–∞–µ–º –≤–∫–ª–∞–¥–∫–∏
tab1, tab2 = st.tabs(["üéØ –û—Ü–µ–Ω–∏—Ç—å –æ–¥–∏–Ω –æ—Ç–≤–µ—Ç", "üìä –û—Ü–µ–Ω–∏—Ç—å —Ñ–∞–π–ª CSV"])

with tab1:
    st.header("–ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞")
    
    col1, col2 = st.columns(2)
    with col1:
        question_number = st.selectbox("‚Ññ –≤–æ–ø—Ä–æ—Å–∞:", [1, 2, 3, 4], key="question_number")
    with col2:
        max_score = {1: 1.0, 2: 2.0, 3: 1.0, 4: 2.0}.get(question_number, 2.0)
        st.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª: {max_score}")
    
    question_text = st.text_area(
        "–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞:",
        height=100,
        placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞...",
        key="question_text"
    )
    
    transcription_text = st.text_area(
        "–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞:",
        height=150,
        placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–∞...",
        key="transcription_text"
    )

    if st.button("‚ö° –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞", type="primary", key="single"):
        if question_text.strip() and transcription_text.strip():
            with st.spinner("ü§ñ –ú–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç..."):
                start_time = time.time()
                final_score, raw_score = grader.predict_single_fast(question_text, transcription_text, question_number)
                processing_time = time.time() - start_time
            
            st.success(f"**–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {final_score} / {int(max_score)}** (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞ {processing_time:.3f} —Å–µ–∫)")
            
            col1, col2 = st.columns([1, 3])
            with col1:
                st.metric("–û—Ü–µ–Ω–∫–∞", f"{final_score}/{int(max_score)}")
            with col2:
                st.progress(final_score / max_score)
            
            with st.expander("üîç –î–µ—Ç–∞–ª–∏"):
                st.write(f"**–°—ã—Ä–∞—è –æ—Ü–µ–Ω–∫–∞:** {raw_score:.4f}")
                st.write(f"**–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:** {processing_time:.3f} —Å–µ–∫")
        else:
            st.warning("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–æ–ª—è.")

with tab2:
    st.header("‚ö° GPU-—É—Å–∫–æ—Ä–µ–Ω–Ω–∞—è –ø–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞")
    st.markdown("""
    **–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ GPU-–≤–µ—Ä—Å–∏–∏:**
    - üöÄ –£—Å–∫–æ—Ä–µ–Ω–∏–µ 10-20x –∑–∞ —Å—á–µ—Ç –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏
    - üéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è GPU/CPU
    - üìä –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä —Å —Ä–∞—Å—á–µ—Ç–æ–º –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏
    - ‚ö° –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: –¥–æ 100+ –æ—Ç–≤–µ—Ç–æ–≤/—Å–µ–∫
    """)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    st.subheader("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        batch_size = st.slider(
            "–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞:",
            min_value=16,
            max_value=256,
            value=128,
            help="–ë–æ–ª—å—à–∏–π —Ä–∞–∑–º–µ—Ä = –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ GPU"
        )
    with col2:
        max_length = st.slider(
            "–ú–∞–∫—Å. –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞:",
            min_value=256,
            max_value=512,
            value=384,
            help="–£–º–µ–Ω—å—à–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É"
        )
    with col3:
        device_info = "GPU" if grader.device.type == 'cuda' else "CPU"
        st.metric("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", device_info)
        if grader.device.type == 'cuda':
            st.info(f"–ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    uploaded_file = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ CSV-—Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ —ç–∫–∑–∞–º–µ–Ω–∞", 
        type=['csv', 'txt'],
        key="file_uploader"
    )
    
    if uploaded_file is not None:
        try:
            # –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
            with st.spinner("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª..."):
                df = safe_read_csv(uploaded_file)
            
            st.success(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            required_columns = ['‚Ññ –≤–æ–ø—Ä–æ—Å–∞', '–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞', '–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                st.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing_columns)}")
                st.info(f"üìã –ù–∞–π–¥–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏: {', '.join(df.columns)}")
            else:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö
                st.subheader("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º:**")
                    question_stats = df['‚Ññ –≤–æ–ø—Ä–æ—Å–∞'].value_counts().sort_index()
                    for q_num, count in question_stats.items():
                        max_score = {1: 1.0, 2: 2.0, 3: 1.0, 4: 2.0}.get(q_num, 2.0)
                        st.write(f"- –í–æ–ø—Ä–æ—Å {q_num}: {count} –æ—Ç–≤–µ—Ç–æ–≤ (–º–∞–∫—Å. {max_score} –±–∞–ª–ª–æ–≤)")
                
                with col2:
                    st.write("**–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:**")
                    display_cols = ['‚Ññ –≤–æ–ø—Ä–æ—Å–∞', '–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞', '–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞']
                    if '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞' in df.columns:
                        display_cols.append('–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞')
                    st.dataframe(df[display_cols].head(3))
                
                # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                estimated_time = len(df) / 50  # –û—Ü–µ–Ω–∫–∞ 50 –æ—Ç–≤–µ—Ç–æ–≤/—Å–µ–∫
                st.info(f"‚è±Ô∏è –û—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {estimated_time/60:.1f} –º–∏–Ω—É—Ç")
                
                if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å GPU-—É—Å–∫–æ—Ä–µ–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É", type="primary", key="batch"):
                    with st.spinner("‚ö° –ó–∞–ø—É—Å–∫–∞–µ–º GPU-—É—Å–∫–æ—Ä–µ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É..."):
                        result_df = grader.predict_batch_gpu_optimized(
                            df, 
                            batch_size=batch_size, 
                            max_length=max_length
                        )
                    
                    if result_df is not None:
                        st.balloons()
                        st.subheader("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏")
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                        display_columns = ['‚Ññ –≤–æ–ø—Ä–æ—Å–∞', '–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞', '–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞', '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted']
                        if '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞' in result_df.columns:
                            display_columns.insert(3, '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞')
                        
                        st.dataframe(result_df[display_columns].head(10))
                        
                        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                        st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ü–µ–Ω–æ–∫")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            avg_grade = result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted'].mean()
                            st.metric("–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞", f"{avg_grade:.2f}")
                        with col2:
                            min_grade = result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted'].min()
                            st.metric("–ú–∏–Ω. –æ—Ü–µ–Ω–∫–∞", f"{min_grade:.2f}")
                        with col3:
                            max_grade = result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted'].max()
                            st.metric("–ú–∞–∫—Å. –æ—Ü–µ–Ω–∫–∞", f"{max_grade:.2f}")
                        with col4:
                            total_count = len(result_df)
                            st.metric("–í—Å–µ–≥–æ –æ—Ç–≤–µ—Ç–æ–≤", total_count)
                        
                        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                        st.subheader("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫")
                        grade_counts = result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted'].value_counts().sort_index()
                        st.bar_chart(grade_counts)
                        
                        # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ
                        st.subheader("üíæ –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
                        csv_result = result_df.to_csv(index=False, sep=';').encode('utf-8')
                        st.download_button(
                            label="üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (CSV)",
                            data=csv_result,
                            file_name="graded_results_gpu.csv",
                            mime="text/csv",
                            key="download_full"
                        )
                        
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {e}")

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
with st.sidebar:
    st.header("‚ö° GPU –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
    st.markdown("""
    **–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:**
    - üéØ –ë–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ 256 –ø—Ä–∏–º–µ—Ä–æ–≤
    - üî• Mixed Precision (float16)
    - ‚ö° Torch Compile
    - üöÄ CUDA Graphs (–∞–≤—Ç–æ)
    - üìä –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
    """)
    
    st.header("üìä –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
    if grader.device.type == 'cuda':
        st.success(f"GPU: {torch.cuda.get_device_name()}")
        st.info(f"–ü–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        st.warning("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU —Ä–µ–∂–∏–º")
    
    st.header("üéØ –û–∂–∏–¥–∞–µ–º–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å")
    st.markdown("""
    - **10,000 –æ—Ç–≤–µ—Ç–æ–≤**: ~2-3 –º–∏–Ω—É—Ç—ã
    - **1,000 –æ—Ç–≤–µ—Ç–æ–≤**: ~10-15 —Å–µ–∫—É–Ω–¥  
    - **100 –æ—Ç–≤–µ—Ç–æ–≤**: ~1-2 —Å–µ–∫—É–Ω–¥—ã
    - **1 –æ—Ç–≤–µ—Ç**: ~0.01 —Å–µ–∫—É–Ω–¥—ã
    """)

# –§—É—Ç–µ—Ä
st.markdown("---")
st.markdown(
    "**‚ö° GPU-—É—Å–∫–æ—Ä–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ —ç–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤** ‚Ä¢ "
    "–£—Å–∫–æ—Ä–µ–Ω–∏–µ 10-20x ‚Ä¢ "
    "MAE: 0.26"
)


