import streamlit as st
import pandas as pd
import numpy as np
import time
import os
from .utils import clean_html_simple, get_model_path, check_model_files

class RussianExamGrader:
    def __init__(self, model_path=None):
        if model_path is None:
            model_path = get_model_path()
            
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.device = "CPU"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏
        existing_files, missing_files = check_model_files(model_path)
        
        if existing_files:
            st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏: {', '.join(existing_files)}")
        
        if missing_files:
            st.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã: {', '.join(missing_files)}")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã
        if any(f in existing_files for f in ['model.safetensors', 'pytorch_model.bin']) and 'config.json' in existing_files:
            try:
                self._load_model()
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                st.info("üí° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ–º–æ-—Ä–µ–∂–∏–º")
        else:
            st.info("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ–º–æ-—Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏")
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ ML –º–æ–¥–µ–ª–∏"""
        try:
            import torch
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            
            st.info("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
                st.success(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {torch.cuda.get_device_name()}")
            else:
                self.device = torch.device('cpu')
                st.info("üíª –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)
            self.model.to(self.device)
            self.model.eval()
            
            st.success("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            
        except ImportError as e:
            st.error(f"‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã ML –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {e}")
            st.info("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch transformers")
            st.info("üí° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ–º–æ-—Ä–µ–∂–∏–º")
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            st.info("üí° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ–º–æ-—Ä–µ–∂–∏–º")
    
    def predict_single_fast(self, question_text, transcription_text, question_number):
        """–û—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        if self.model is None:
            return self._demo_predict_single(question_text, transcription_text, question_number)
        else:
            return self._ml_predict_single(question_text, transcription_text, question_number)
    
    def _ml_predict_single(self, question_text, transcription_text, question_number):
        """ML –æ—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        try:
            import torch
            
            # –û—á–∏—Å—Ç–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞
            cleaned_question = clean_html_simple(question_text)
            input_text = f"–ó–ê–î–ê–ù–ò–ï: {cleaned_question} | –î–ò–ê–õ–û–ì: {transcription_text}"
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(
                input_text,
                max_length=512,
                padding=True,
                truncation=True,
                return_tensors='pt'
            ).to(self.device)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                outputs = self.model(**inputs)
                raw_score = float(outputs.logits.cpu().numpy()[0][0])
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
            max_score = {1: 1.0, 2: 2.0, 3: 1.0, 4: 2.0}.get(question_number, 2.0)
            final_score = int(round(np.clip(raw_score, 0, max_score)))
            
            return final_score, raw_score
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ ML –æ—Ü–µ–Ω–∫–∏: {e}")
            return self._demo_predict_single(question_text, transcription_text, question_number)
    
    def _demo_predict_single(self, question_text, transcription_text, question_number):
        """–î–µ–º–æ-–æ—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        time.sleep(0.05)
        
        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –¥–µ–º–æ-–æ—Ü–µ–Ω–∫–∞
        text_length = len(transcription_text)
        word_count = len(transcription_text.split())
        base_score = min(2.0, word_count / 20)
        random_factor = np.random.normal(0, 0.3)
        raw_score = max(0, min(2.0, base_score + random_factor))
        
        max_score = {1: 1.0, 2: 2.0, 3: 1.0, 4: 2.0}.get(question_number, 2.0)
        final_score = int(round(np.clip(raw_score, 0, max_score)))
        
        return final_score, float(raw_score)
    
    def predict_batch_gpu_optimized(self, df, batch_size=100, max_length=384):
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞"""
        if self.model is None:
            return self._demo_predict_batch(df, batch_size)
        else:
            return self._ml_predict_batch(df, batch_size, max_length)
    
    def _ml_predict_batch(self, df, batch_size, max_length):
        """ML –ø–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞"""
        try:
            import torch
            
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
            df_copy = df.copy()
            mask = ~(df_copy['–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞'].isna() | df_copy['–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞'].isna())
            df_copy = df_copy[mask].copy()
            
            df_copy['–¢–µ–∫—Å—Ç_–æ—á–∏—â–µ–Ω–Ω—ã–π'] = df_copy['–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞'].apply(clean_html_simple)
            df_copy['Input_Text'] = "–ó–ê–î–ê–ù–ò–ï: " + df_copy['–¢–µ–∫—Å—Ç_–æ—á–∏—â–µ–Ω–Ω—ã–π'] + " | –î–ò–ê–õ–û–ì: " + df_copy['–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞']
            
            texts = df_copy['Input_Text'].tolist()
            question_numbers = df_copy['‚Ññ –≤–æ–ø—Ä–æ—Å–∞'].values
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            all_predictions = []
            total_samples = len(texts)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏
            for i in range(0, total_samples, batch_size):
                batch_texts = texts[i:i + batch_size]
                current_batch_size = len(batch_texts)
                
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–∞
                inputs = self.tokenizer(
                    batch_texts,
                    max_length=max_length,
                    padding=True,
                    truncation=True,
                    return_tensors="pt"
                ).to(self.device)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    batch_predictions = outputs.logits.squeeze()
                
                if batch_predictions.ndim == 0:
                    batch_predictions = [float(batch_predictions.cpu().numpy())]
                elif batch_predictions.ndim == 1:
                    batch_predictions = batch_predictions.cpu().numpy().tolist()
                else:
                    batch_predictions = batch_predictions.cpu().numpy().flatten().tolist()
                
                all_predictions.extend(batch_predictions[:current_batch_size])
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                progress = min((i + current_batch_size) / total_samples, 1.0)
                progress_bar.progress(progress)
                status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {min(i + current_batch_size, total_samples)}/{total_samples}")
            
            progress_bar.empty()
            status_text.empty()
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
            result_df = df.iloc[df_copy.index].copy() if len(df_copy) < len(df) else df.copy()
            result_df['predicted_score'] = all_predictions
            
            def finalize_score(row):
                score = row['predicted_score']
                question_num = row['‚Ññ –≤–æ–ø—Ä–æ—Å–∞']
                max_score = {1: 1.0, 2: 2.0, 3: 1.0, 4: 2.0}.get(question_num, 2.0)
                return int(round(np.clip(score, 0, max_score)))
            
            result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted'] = result_df.apply(finalize_score, axis=1)
            
            st.success(f"‚úÖ ML –æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_samples} –æ—Ç–≤–µ—Ç–æ–≤")
            return result_df.drop(columns=['predicted_score'], errors='ignore')
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ ML –ø–∞–∫–µ—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏: {e}")
            return self._demo_predict_batch(df, batch_size)
    
    def _demo_predict_batch(self, df, batch_size):
        """–î–µ–º–æ –ø–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞"""
        try:
            # –î–µ–º–æ-–æ–±—Ä–∞–±–æ—Ç–∫–∞
            result_df = df.copy()
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            total_rows = len(result_df)
            
            for i in range(total_rows):
                # –î–µ–º–æ-–æ—Ü–µ–Ω–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
                transcription = str(result_df.iloc[i]['–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞'])
                word_count = len(transcription.split())
                base_score = min(2.0, word_count / 25)
                random_factor = np.random.normal(0, 0.2)
                raw_score = max(0, min(2.0, base_score + random_factor))
                
                question_num = result_df.iloc[i]['‚Ññ –≤–æ–ø—Ä–æ—Å–∞']
                max_score = {1: 1.0, 2: 2.0, 3: 1.0, 4: 2.0}.get(question_num, 2.0)
                final_score = int(round(np.clip(raw_score, 0, max_score)))
                
                result_df.loc[result_df.index[i], '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞_predicted'] = final_score
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                if i % 10 == 0 or i == total_rows - 1:
                    progress = (i + 1) / total_rows
                    progress_bar.progress(progress)
                    status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i+1}/{total_rows} –æ—Ç–≤–µ—Ç–æ–≤")
            
            progress_bar.empty()
            status_text.empty()
            
            st.success(f"‚úÖ –î–µ–º–æ-–æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_rows} –æ—Ç–≤–µ—Ç–æ–≤")
            return result_df
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –¥–µ–º–æ-–æ—Ü–µ–Ω–∫–∏: {e}")
            return None
